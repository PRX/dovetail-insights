-- Dovetail Insights: Dimensional

DECLARE range_start TIMESTAMP DEFAULT '<%= composition.abs_from.to_s %>';
DECLARE range_end TIMESTAMP DEFAULT '<%= composition.abs_to.to_s %>';

<%
  # NOTE: Rolling window calculations and ranges are *unused* with non-rolling
  # compositions, but included to make this template less complex with fewer
  # conditionals.
%>

<%
  # I couldn't get any of the native range functions in BigQuery to do
  # exactly what I wanted, so this is some manual math. The goal is to
  # have a set of ranges, anchored to the chosen range end, and starting
  # at or before the chosen range start, such that each range is exactly
  # the same number of seconds
%>
DECLARE rolling_window_num_seconds INT64 DEFAULT <%= composition.try(:window) || 86400 %>;

<%# Get the total time range duration in seconds %>
DECLARE range_duration_in_seconds INT64 DEFAULT TIMESTAMP_DIFF(range_end, range_start, SECOND);

<%
  # Figure out how many windows for the current rolling window setting
  # fit into that duration (e.g., duration=10, window=3, that would give
  # 3.333 windows)
  # This assume windows are always defined as some number of *days*
%>
DECLARE num_windows_in_range FLOAT64 DEFAULT range_duration_in_seconds / rolling_window_num_seconds;

<%
  # Round the number of windows up, so we have a whole number of windows
  # that covers at least the entire duration (e.g., 4 windows)
%>
DECLARE expanded_num_windows_in_range INT64 DEFAULT CAST(CEIL(num_windows_in_range) AS INT64);

<%# Get the total number of seconds in that whole number quantity of windows %>
DECLARE expanded_range_duration_in_seconds INT64 DEFAULT expanded_num_windows_in_range * rolling_window_num_seconds;


DECLARE expanded_range_start TIMESTAMP DEFAULT TIMESTAMP_SUB(range_end, INTERVAL expanded_range_duration_in_seconds SECOND);

<%
  # In the initial downloads CTE, only include columns that are required to
  # complete the query. Look for any columns on the downloads table that are
  # required directly, or any columns that are needed to JOIN other table. This
  # reduces the amount of data processes, which saves a significant amount of
  # money.

  dl_cte_selects = ["timestamp", "request_uuid"]

  group_and_filter_dimension_names = []
  group_and_filter_dimension_names.concat(composition.groups.map {|g| g.dimension.to_s }) if composition.groups
  group_and_filter_dimension_names.concat(composition.filters.map {|f| f.dimension.to_s }) if composition.filters

  group_and_filter_dimension_names.each do |dimension_name|
    dimension_def = DataSchema.dimensions[dimension_name]
    req_tables = dimension_def["BigQuery"]["RequiredTables"]
    req_tables&.filter {|t| t != "downloads" }&.each do |table_name|
      table_def = DataSchema.tables[table_name]
      if table_def.dig("BigQuery", "JoinsTo", "downloads", "Key")
        frn_key = table_def.dig("BigQuery", "JoinsTo", "downloads", "Key")

        dl_cte_selects << frn_key
      end
    end

    dl_cte_selects.concat(dimension_def["BigQuery"]["RequiredColumns"]) if dimension_def["BigQuery"]["RequiredColumns"]
  end

  composition.metrics.each do |metric|
    metric_def = DataSchema.metrics[metric.metric.to_s]
    dl_cte_selects.concat(metric_def["BigQuery"]["RequiredColumns"]) if metric_def["BigQuery"]["RequiredColumns"]
  end
%>

<%
  # In order to ensure that the time range operates as expected, we make
  # queries for downloads and impressions before doing any JOINs.
%>
WITH
downloads_in_range AS (
  SELECT <%= dl_cte_selects.uniq.join(", ") %>

  FROM production.dt_downloads AS downloads

  WHERE
    downloads.timestamp >= range_start AND downloads.timestamp < range_end
    AND downloads.is_duplicate = FALSE
    AND downloads.feeder_podcast IS NOT NULL
  )

<% if joins.uniq.join(" ").include?("impressions_in_range") %>
  ,impressions_in_range AS (
    -- TODO determine selects for this like we did with downloads
    SELECT *

    FROM production.dt_impressions AS impressions

    WHERE
      impressions.timestamp >= range_start AND impressions.timestamp < range_end
      AND impressions.is_duplicate = FALSE
      AND impressions.feeder_podcast IS NOT NULL
  )
<% end %>

<%
  # This generates a set of ranges to be used on time series lenses, when
  # the granularity is a rolling window. It's pretty safe to set up the
  # CTE on all queries. This uses the window math DEFINEs from above, to
  # create a table with a row for each window, with a column for the start
  # and end.
%>
,rolling_window_ranges AS (
  SELECT *

  FROM UNNEST(
    GENERATE_RANGE_ARRAY(
      RANGE(expanded_range_start, range_end),
      INTERVAL rolling_window_num_seconds SECOND
    )
  ) AS rolling_window_range
)

SELECT <%= selects.join(",\n\t") %>
<%# TODO get this AS name from the schema %>
FROM downloads_in_range AS downloads

<%= joins.uniq.join("\n") %>

WHERE <%= wheres.join("\n\tAND ") %>

<%= "GROUP BY #{group_bys.join(",\n\t")}" if group_bys.present? %>
