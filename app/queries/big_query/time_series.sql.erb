<% podcast_filter = filters.find {|f| f.dimension == :podcast_id } %>

-- Pre calculate joins, so we know all the tables we need as early as possible

-- Need to JOIN on any tables that are needed for SELECTs, GROUP BYs, or WHEREs
-- figure out how to choose the best one to use given the rest of the query
-- TODO Needs to handle complex join conditions, like RANGE_CONTAINS
<%
  tables = []

  # Collect all the tables that are needed for filters, metrics, and groups.
  # This only includes the tables that are *directly* required for each of
  # these; those tables may have chained dependencies, which are handled below.
  filters.each { |filter| (DataSchema.dimensions[filter.dimension.to_s]["BigQuery"]["RequiredTables"] || []).each { |t| tables << t } }
  metrics.each { |metric| (DataSchema.metrics[metric.metric.to_s]["BigQuery"]["RequiredTables"] || []).each { |t| tables << t } }
  groups.each { |group| (DataSchema.dimensions[group.dimension.to_s]["BigQuery"]["RequiredTables"] || []).each { |t| tables << t } }

  joins = []

  def all_joins_for_table(table_name)
    all_joins = []

    unless table_name == "downloads"
      table_def = DataSchema.tables[table_name]
      bq = table_def["BigQuery"]

      bq["JoinsTo"].each do |join_table_name, join_def|
        unless join_table_name == "downloads"
          all_joins.concat(all_joins_for_table(join_table_name))
        end

        if join_def["Key"]
          all_joins << "#{bq["Join"]} JOIN #{bq["Table"]} AS #{table_name} ON #{table_name}.#{bq["Key"]} = #{join_table_name}.#{join_def["Key"]}"
        elsif join_def["Expression"]
          all_joins << "#{bq["Join"]} JOIN #{bq["Table"]} AS #{table_name} ON #{join_def["Expression"]}"
        end
      end
    end

    all_joins
  end

  tables.uniq.each do |table_name|
    joins.concat(all_joins_for_table(table_name))
  end

  if rolling?
    joins << "LEFT JOIN rolling_window_ranges ON RANGE_CONTAINS(rolling_window_range, downloads.timestamp)"
  end
%>

DECLARE range_start TIMESTAMP DEFAULT '<%= (defined?(compare_abs_from) ? compare_abs_from : abs_from).to_s %>';
DECLARE range_end TIMESTAMP DEFAULT '<%= (defined?(compare_abs_to) ? compare_abs_to : abs_to).to_s %>';

<% if rolling? %>
  -- I couldn't get any of the native range functions in BigQuery to do
  -- exactly what I wanted, so this is some manual math. The goal is to
  -- have a set of ranges, anchored to the chosen range end, and starting
  -- at or before the chosen range start, such that each range is exactly
  -- the same number of seconds
  DECLARE rolling_window_num_seconds INT64 DEFAULT <%= window %>;

  -- Get the total time range duration in seconds
  DECLARE range_duration_in_seconds INT64 DEFAULT TIMESTAMP_DIFF(range_end, range_start, SECOND);

  -- Figure out how many windows for the current rolling window setting
  -- fit into that duration (e.g., duration=10, window=3, that would give
  -- 3.333 windows)
  -- This assume windows are always defined as some number of *days*
  DECLARE num_windows_in_range FLOAT64 DEFAULT range_duration_in_seconds / rolling_window_num_seconds;
  -- Round the number of windows up, so we have a whole number of windows
  -- that covers at least the entire duration (e.g., 4 windows)
  DECLARE expanded_num_windows_in_range INT64 DEFAULT CAST(CEIL(num_windows_in_range) AS INT64);

  -- Get the total number of seconds in that whole number quantity of
  -- windows
  DECLARE expanded_range_duration_in_seconds INT64 DEFAULT expanded_num_windows_in_range * rolling_window_num_seconds;

  --
  DECLARE expanded_range_start TIMESTAMP DEFAULT TIMESTAMP_SUB(range_end, INTERVAL expanded_range_duration_in_seconds SECOND);
<% end %>

-- In order to ensure that the time range operates as expected, we make
-- queries for downloads and impressions before doing any JOINs. These
-- require a selection of podcast IDs, which is mostly just a safety
-- valve during development, to make sure I don't accidentally make a lot
-- of hugely expesnive queries
-- TODO Apply filters here when possible to make things more efficient
WITH
downloads_in_range AS (
  SELECT *

  FROM production.dt_downloads AS downloads

  WHERE
    downloads.timestamp >= range_start AND downloads.timestamp < range_end
    AND downloads.is_duplicate = FALSE
    AND downloads.feeder_podcast IS NOT NULL
    AND downloads.feeder_podcast IN (<%= podcast_filter&.values&.join(", ") %>)
  )

<% if joins.uniq.join(" ").include?("impressions_in_range") %>
  ,impressions_in_range AS (
    SELECT *

    FROM production.dt_impressions AS impressions

    WHERE
      impressions.timestamp >= range_start AND impressions.timestamp < range_end
      AND impressions.is_duplicate = FALSE
      AND impressions.feeder_podcast IS NOT NULL
      AND impressions.feeder_podcast IN (<%= podcast_filter&.values&.join(", ") %>)
  )
<% end %>

<% if rolling? %>
  ,rolling_window_ranges AS (
    -- This generates a set of ranges to be used on time series lenses, when
    -- the granularity is a rolling window. It's pretty safe to set up the
    -- CTE on all queries. This uses the window math DEFINEs from above, to
    -- create a table with a row for each window, with a column for the start
    -- and end.
    SELECT *

    FROM UNNEST(
      GENERATE_RANGE_ARRAY(
        RANGE(expanded_range_start, range_end),
        INTERVAL rolling_window_num_seconds SECOND
      )
    ) AS rolling_window_range
  )
<% end %>

-- SELECT ----------------------------------------------------------------------

<%
  selects = []

  if rolling?
    selects << "rolling_window_range AS #{granularity_as}_raw"
    selects << %Q(FORMAT_TIMESTAMP("%Y-%m-%dT%H:%M:%SZ", RANGE_END(rolling_window_range), "UTC") AS #{granularity_as})
  else
    bq_granularity = case granularity
    when :daily
      "DAY"
    when :weekly
      "WEEK(SUNDAY)"
    when :monthly
      "MONTH"
    when :quarterly
      "QUARTER"
    when :yearly
      "YEAR"
    end

    selects << "DATE(TIMESTAMP_TRUNC(downloads.timestamp, #{bq_granularity})) AS #{granularity_as}"
  end

  # Add SELECTs derived from the group selections and the modes they are
  # operating in
  #
  # All of these SELECTs use a unique, derived AS, so there is no risk of
  # collision, but they are not otherwise deduped, so it is currently possible
  # for things to be SELECTed more than once unnecessarily
  groups.each do |group|
    dimension_def = DataSchema.dimensions[group.dimension.to_s]
    selector = dimension_def["BigQuery"]["Selector"]

    if group.extract
      extract_arg = {
        hour: "HOUR",
        day_of_week: "DAYOFWEEK",
        day: "DAY",
        week: "WEEK",
        month: "MONTH",
        year: "YEAR"
      }[group.extract]
      selects << "EXTRACT(#{extract_arg} FROM #{selector}) AS #{group.as}"
    elsif group.truncate
      truncate_arg = {
        week: "WEEK",
        month: "MONTH",
        year: "YEAR"
      }[group.truncate]
      # Format after truncating to get a format we are anticipating
      selects << %Q(FORMAT_TIMESTAMP("%Y-%m-%d %H:%M:%S %Ez", TIMESTAMP_TRUNC(#{selector}, #{truncate_arg}, "UTC"), "UTC") AS #{group.as})
    elsif group.indices
      cases = []

      group.abs_indices.each_with_index do |i, idx|
        if dimension_def["BigQuery"]["Type"] == "TIMESTAMP"
          # Wrap TIMESTAMPS in quotes
          cases << "WHEN #{selector} < '#{i}' THEN '#{group.abs_indices[idx]}'"
        else
          cases << "WHEN #{selector} < #{i} THEN '#{group.abs_indices[idx]}'"
        end
      end

      cases << "ELSE '#{Compositions::Components::Group::TERMINATOR_INDEX}'"

      selects << "CASE \n #{cases.join("\n")} \n END AS #{group.as}"
    else
      selects << "#{selector} AS #{group.as}"
    end

    # Select the exhibit property for this dimension if necessary
    if dimension_def["ExhibitProperty"]
      exhibit_property_name = dimension_def["ExhibitProperty"]
      property_def = DataSchema.dimensions[exhibit_property_name] || DataSchema.properties[exhibit_property_name]
      exhibit_selector = property_def["BigQuery"]["Selector"]
      exhibit_as = "#{group.as}_#{exhibit_property_name}"

      selects << "#{exhibit_selector} AS #{exhibit_as}"
    end
  end

  # -- TODO Needs to handle variables/uniques
  # With a valid composition, there should never be any duplication of these
  metrics.each do |metric|
    metric_def = DataSchema.metrics[metric.metric.to_s]
    selector = metric_def["BigQuery"]["Selector"]

    if metric_def["Type"] && metric_def["Type"] == "Variable"

    else
      selects << "#{selector} AS #{metric.as}"
    end
  end
%>

SELECT <%= selects.join(", ") %>

-- END SELECT ------------------------------------------------------------------

-- FROM ------------------------------------------------------------------------

-- TODO get this AS name from the schema
FROM downloads_in_range AS downloads

-- END FROM --------------------------------------------------------------------

-- JOINS -----------------------------------------------------------------------

<%= joins.uniq.join("\n") %>

-- END JOINS -------------------------------------------------------------------

-- WHERE -----------------------------------------------------------------------

<%
  # WHEREs are added from filters, and only from filters. Different types of
  # filters result in different types of WHERE clauses, to support things like
  # lists, ranges, etc. Some values in WHEREs need to be quoted.
  wheres = []

  filters.each do |filter|
    filter_def = DataSchema.dimensions[filter.dimension.to_s]
    selector = filter_def["BigQuery"]["Selector"]
    bq_type = filter_def["BigQuery"]["Type"]

    # Filters with a `from` are ranges
    if filter.from
      # Wrap values for timestamp fields in quotes
      from = %Q("#{filter.abs_from}")
      to = %Q("#{filter.abs_to}")

      if filter.operator == :include
        wheres << "#{selector} >= #{from} AND #{selector} < #{to}"
      else
        wheres << "(#{selector} < #{from} OR #{selector} >= #{to})"
      end
    elsif filter.gte
      if filter.operator == :include
        wheres << "#{selector} >= #{filter.gte} AND #{selector} < #{filter.lt}"
      else
        wheres << "(#{selector} < #{filter.gte} OR #{selector} >= #{filter.lt})"
      end
    else
      # Filter on a basic list of values
      op = filter.operator == :include ? "IN" : "NOT IN"

      if bq_type == "STRING"
        wrapped_values = filter.values.map { |v| %Q("#{v}") }
        wheres << "#{selector} #{op} (#{wrapped_values.join(", ")})"
      else
        wheres << "#{selector} #{op} (#{filter.values.join(", ")})"
      end
    end
  end
%>

WHERE <%= wheres.join(" AND ") %>

-- END WHERE -------------------------------------------------------------------

-- GROUP BY --------------------------------------------------------------------

<%
  # Each selected group adds a GROUP BY clause. Groups are the only source of
  # GROUP BY clauses for dimensional lens, but some dimensions add multiple
  # GROUP BY clauses, to support exhibit properties
  group_bys = []

  group_bys << "#{granularity_as}_raw" if rolling?
  group_bys << granularity_as

  groups.each do |group|
    group_bys << group.as

    dimension_def = DataSchema.dimensions[group.dimension.to_s]
    if dimension_def["ExhibitProperty"]
      exhibit_property_name = dimension_def["ExhibitProperty"]
      exhibit_as = "#{group.as}_#{exhibit_property_name}"

      group_bys << exhibit_as
    end
  end
%>

<%= "GROUP BY #{group_bys.join(", ")}" if group_bys.present? %>

-- END GROUP BY ----------------------------------------------------------------
